{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da883fe2-b798-4085-a202-b8ee16ca4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd883e-69d5-456e-aaf1-9570c0223850",
   "metadata": {},
   "source": [
    "Debido a la estructura del archivo, debería leerse dos veces para poder calcular desde \n",
    "antes las dimensiones finales de los arrays a cargar. Si el archivo es masivo (>500.000 entradas)\n",
    "es importante evitar concatenar arrays (numpy) o tablas (pandas) dado que ese proceso demanda RAM.\n",
    "\n",
    "Para comenzar, la rutina que lo explora y carga todas las dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f873c59-e0ef-4ab8-9d7a-1248f6ad5216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_datainfo(filename):\n",
    "    \"\"\"\n",
    "    Rutina para leer la información (dimensiones totales) de un archivo NDskl_ascii\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        ## esto puede ser un problema si el archivo es muy grande\n",
    "        #lines = f.readlines()\n",
    "        ## Lo cambio por una lectura progresiva:\n",
    "\n",
    "        # 1. Cargamos ndims para crear objetos de tamaño adecuado:\n",
    "        _ = f.readline()\n",
    "        d['Ndims'] = int(f.readline().split()[0])\n",
    "\n",
    "        # 2. Nos saltamos los comentarios:\n",
    "        line = f.readline()\n",
    "        while '#' == line[0]: line = f.readline()\n",
    "\n",
    "        # 3. Guardamos las dimensiones espaciales:\n",
    "        if \"BBOX\" in line.split()[0]:\n",
    "            d[\"BBOX_min\"] = [float(i) for i in line.split()[1].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "            d[\"BBOX_max\"] = [float(i) for i in line.split()[2].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "\n",
    "        # 4.1. Leemos la cantidad de puntos críticos:\n",
    "        while \"[CRITICAL POINTS]\" not in line: line = f.readline()\n",
    "        d[\"CriticalPoints\"] = int(f.readline().split()[0])\n",
    "\n",
    "        # 4.2 Por cada punto critico sumamos su cantidad de filamentos:\n",
    "        totFilsinCP = 0\n",
    "        for _ in range(d[\"CriticalPoints\"]):\n",
    "            _ = f.readline()\n",
    "            nFilsinCP = int(f.readline().split()[0])\n",
    "            totFilsinCP += nFilsinCP\n",
    "            # Nos saltamos esas líneas:\n",
    "            for _ in range(nFilsinCP): _ = f.readline()\n",
    "        d[\"CriticalPoints_NumFils\"] = totFilsinCP\n",
    "\n",
    "        # 5.1 Ahora vamos al bloque de Filamentos, cargando total: \n",
    "        while \"[FILAMENTS]\" not in line: line = f.readline()\n",
    "        d[\"Filaments\"] = int(f.readline().split()[0])\n",
    "\n",
    "        # 5.2 Por cada filamento, sumamos sus puntos conectores:\n",
    "        totPointsinFil = 0\n",
    "        for _ in range(d[\"Filaments\"]):\n",
    "            nPointsinFil = int(f.readline().split()[-1])\n",
    "            totPointsinFil += nPointsinFil\n",
    "            # Nos saltamos estas líneas:\n",
    "            for _ in range(nPointsinFil): _ = f.readline()\n",
    "        d[\"Filaments_NumPoints\"] = totPointsinFil\n",
    "                \n",
    "        # 6.1 Ahora cargaremos la sección CRITICAL POINTS DATA\n",
    "        # Esta es simplemente una Tabla con dims: N_CP x NF, donde\n",
    "        # NF es el número de columnas (primera info que leeremos):\n",
    "        while \"[CRITICAL POINTS DATA]\" not in line: line = f.readline()\n",
    "        d[\"CriticalPointsData_NF\"] = int(f.readline().split()[0])\n",
    "        d[\"CriticalPointsData_Fields\"] = []\n",
    "        for _ in range(d[\"CriticalPointsData_NF\"]): \n",
    "            d[\"CriticalPointsData_Fields\"].append(f.readline().strip())\n",
    "\n",
    "        # 6.2 Ahora que tenemos los NF y sus nombres, nos saltamos los registros:\n",
    "        for _ in range(d[\"CriticalPoints\"]): _ = f.readline()\n",
    "\n",
    "        # 7.1 Ahora cargaremos la sección FILAMENTS DATA\n",
    "        # Esta es simplemente una Tabla con dims: N_Fil x NF, donde\n",
    "        # NF es el número de columnas de esta tabla (primera info que leeremos):\n",
    "        while \"[FILAMENTS DATA]\" not in line: line = f.readline()\n",
    "        d[\"Filaments_NF\"] = int(f.readline().split()[0])\n",
    "        d[\"Filaments_Fields\"] = []\n",
    "        for _ in range(d[\"Filaments_NF\"]): \n",
    "            d[\"Filaments_Fields\"].append(f.readline().strip())\n",
    "\n",
    "        # 7.2 Ahora que tenemos los NF y sus nombres, nos saltamos los registros:\n",
    "        # Ojo que esta tabla tiene todos los puntos conectores de todos los filamentos, en orden:\n",
    "        for _ in range(d[\"Filaments_NumPoints\"]): _ = f.readline()\n",
    "        # Done! \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20666224-3dd6-4930-bbea-a47ac683b435",
   "metadata": {},
   "source": [
    "Ahora, utilizando la misma anterior como punto de partida, completaremos las seccionas que nos saltamos la primera vez e iremos guardando en arrays y tablas. \n",
    "Para hacer este código modular (y extensible) desde el inicio, agregaremos un parámetro opcional que permita seleccionar qué datos del archivo se quiere cargar, y condicionaremos cada parte a estos tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f96f2572-6f07-4362-8cd7-35b16bf22cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_NDskl_ascii(filename, fields=[\"[CRITICAL POINTS]\",\"[FILAMENTS]\",\"[CRITICAL POINTS DATA]\", \"[FILAMENTS DATA]\"]):\n",
    "    \"\"\"\n",
    "    Rutina para leer la información (dimensiones totales) de un archivo NDskl_ascii\n",
    "\n",
    "    Params:\n",
    "        - fields (opcional): Permite elegir qué campos del archivo se quieren cargar (todos por defecto).\n",
    "                             Recibe una lista donde cada elemento almacena el nombre del campo deseado.\n",
    "    \"\"\"\n",
    "    # Primero obtenemos la informacion del archivo:\n",
    "    ndinfo = get_datainfo(filename)\n",
    "\n",
    "    # Armamos la estrutura del contenedor\n",
    "    d = {\"METADATA\":ndinfo}\n",
    "    d[\"CRITICAL_POINTS\"] = {}\n",
    "    d[\"FILAMENTS\"] = {} \n",
    "    d[\"CRITICAL_POINTS_DATA\"] = None\n",
    "    d[\"FILAMENTS_DATA\"] = None\n",
    "\n",
    "    # Ahora leemos cada parte \n",
    "    with open(filename, \"r\") as f:\n",
    "\n",
    "        Ndims = ndinfo[\"Ndims\"]\n",
    "\n",
    "        # Leemos hasta el primer bloque:\n",
    "        while \"[CRITICAL POINTS]\" not in line: line = f.readline()\n",
    "        if \"[CRITICAL POINTS]\" in fields:\n",
    "            # Creamos nuestros arrays:\n",
    "            # FIXME: Como cada punto comienza con una linea con su info, \n",
    "            # meteré todo en arrays de nympy porque no sé como armarlo \n",
    "            # a saltos por registro en pandas...\n",
    "            d[\"CRITICAL_POINTS\"][\"CPinfo\"] = {}\n",
    "            fd = d[\"CRITICAL_POINTS\"][\"CPinfo\"]\n",
    "            numCP = ndinfo[\"CriticalPoints\"]\n",
    "            fd[\"type\"] = np.empty((numCP,), dtype=np.int16)\n",
    "            fd[\"Pos_0\"] = np.empty((numCP,), dtype=np.float)\n",
    "            fd[\"Pos_1\"] = np.empty((numCP,), dtype=np.float)\n",
    "            fd[\"value\"] = np.empty((numCP,), dtype=np.float)\n",
    "            fd[\"pairID\"] = np.empty((numCP,), dtype=np.int32)\n",
    "            fd[\"boundary\"] = np.empty((numCP,), dtype=np.int16)\n",
    "            # Valores que almacenamos para encontrarlos en la tabla:\n",
    "            fd[\"pairI\"] = np.empty((numCP,), dtype=np.int32)\n",
    "            fd[\"pairID\"] = np.empty((numCP,), dtype=np.int32)\n",
    "            \n",
    "\n",
    "            ## ACA VOY!!!!\n",
    "            return\n",
    "            \n",
    "            d[\"CriticalPoints\"] = int(f.readline().split()[0])\n",
    "\n",
    "            totFilsinCP = 0\n",
    "            for _ in range(d[\"CriticalPoints\"]):\n",
    "                _ = f.readline()\n",
    "                nFilsinCP = int(f.readline().split()[0])\n",
    "                totFilsinCP += nFilsinCP\n",
    "                # Nos saltamos esas líneas:\n",
    "                for _ in range(nFilsinCP): _ = f.readline()\n",
    "            d[\"CriticalPoints_NumFils\"] = totFilsinCP\n",
    "\n",
    "        # 5.1 Ahora vamos al bloque de Filamentos, cargando total: \n",
    "        while \"[FILAMENTS]\" not in line: line = f.readline()\n",
    "        d[\"Filaments\"] = int(f.readline().split()[0])\n",
    "\n",
    "        # 5.2 Por cada filamento, sumamos sus puntos conectores:\n",
    "        totPointsinFil = 0\n",
    "        for _ in range(d[\"Filaments\"]):\n",
    "            nPointsinFil = int(f.readline().split()[-1])\n",
    "            totPointsinFil += nPointsinFil\n",
    "            # Nos saltamos estas líneas:\n",
    "            for _ in range(nPointsinFil): _ = f.readline()\n",
    "        d[\"Filaments_NumPoints\"] = totPointsinFil\n",
    "                \n",
    "        # 6.1 Ahora cargaremos la sección CRITICAL POINTS DATA\n",
    "        # Esta es simplemente una Tabla con dims: N_CP x NF, donde\n",
    "        # NF es el número de columnas (primera info que leeremos):\n",
    "        while \"[CRITICAL POINTS DATA]\" not in line: line = f.readline()\n",
    "        d[\"CriticalPointsData_NF\"] = int(f.readline().split()[0])\n",
    "        d[\"CriticalPointsData_Fields\"] = []\n",
    "        for _ in range(d[\"CriticalPointsData_NF\"]): \n",
    "            d[\"CriticalPointsData_Fields\"].append(f.readline().strip())\n",
    "\n",
    "        # 6.2 Ahora que tenemos los NF y sus nombres, nos saltamos los registros:\n",
    "        for _ in range(d[\"CriticalPoints\"]): _ = f.readline()\n",
    "\n",
    "        # 7.1 Ahora cargaremos la sección FILAMENTS DATA\n",
    "        # Esta es simplemente una Tabla con dims: N_Fil x NF, donde\n",
    "        # NF es el número de columnas de esta tabla (primera info que leeremos):\n",
    "        while \"[FILAMENTS DATA]\" not in line: line = f.readline()\n",
    "        d[\"Filaments_NF\"] = int(f.readline().split()[0])\n",
    "        d[\"Filaments_Fields\"] = []\n",
    "        for _ in range(d[\"Filaments_NF\"]): \n",
    "            d[\"Filaments_Fields\"].append(f.readline().strip())\n",
    "\n",
    "        # 7.2 Ahora que tenemos los NF y sus nombres, nos saltamos los registros:\n",
    "        # Ojo que esta tabla tiene todos los puntos conectores de todos los filamentos, en orden:\n",
    "        for _ in range(d[\"Filaments_NumPoints\"]): _ = f.readline()\n",
    "        # Done! \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49a34675-302d-42ae-84e2-472582091809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Ndims': 2, 'BBOX_min': [0.0, 0.0], 'BBOX_max': [50000.0, 50000.0], 'CriticalPoints': 2116, 'CriticalPoints_NumFils': 4232, 'Filaments': 2116, 'Filaments_NumPoints': 71703, 'CriticalPointsData_NF': 9, 'CriticalPointsData_Fields': ['persistence_ratio', 'persistence_nsigmas', 'persistence', 'persistence_pair', 'parent_index', 'parent_log_index', 'log_field_value', 'field_value', 'cell'], 'Filaments_NF': 5, 'Filaments_Fields': ['field_value', 'orientation', 'cell', 'log_field_value', 'type']}\n"
     ]
    }
   ],
   "source": [
    "### Codigo para testear la rutina:\n",
    "\n",
    "filename = \"simu_2D.ND.NDnet_s3_ascii\"\n",
    "\n",
    "d = get_datainfo(filename)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2febdb-241c-487b-8bef-d9cc2d13a5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0820069-fe8f-4ae6-b0c6-c70458b50064",
   "metadata": {},
   "source": [
    "Acá desarrollé la _python-magick_ de la lectura de la dimensiones en ascii ;-)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de259d73-cc1a-47d3-aaf7-6c5a17b54689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto se lee del archivo:\n",
    "a,b = \"[0,0]\", \"[5000,5000]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22a94ca5-a1e6-4447-9583-eb1db750e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5000.0, 5000.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformación a lista:\n",
    "[float(i) for i in b.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcfd87-982e-4c3f-a33c-421179e668da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cd701-ce62-4c71-a966-ddb021d1da53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad48816-2547-4b4f-85b9-15b9317309ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe876b68-d234-4a5c-a7eb-e77ee0a2c5cf",
   "metadata": {},
   "source": [
    "###  Código incial trabajado en la reunión\n",
    "(para comparar luego. Este extraía la sección de filamentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04ec03dd-95f6-457f-8fd1-90eff8d9afb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 21 (53757026.py, line 26)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwhile True:\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_filaments(filename):\n",
    "    \"\"\"\n",
    "    Rutina para extraer el bloque 1 de filamentos de un archivo NDskl_ascii\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        with open(\"filaments.dat\", \"w\") as fout:\n",
    "            ## esto puede ser un problema si el archivo es muy grande\n",
    "            #lines = f.readlines()\n",
    "            ## Lo cambio por una lectura progresiva:\n",
    "\n",
    "            # 1. Cargamos ndims para crear objetos de tamaño adecuado:\n",
    "            _ = f.readline()\n",
    "            d['Ndims'] = int(f.readline().spli()[0])\n",
    "\n",
    "            # 2. Nos saltamos los comentarios:\n",
    "            line = f.readline()\n",
    "            if '#' == line[0]: line = f.readline()\n",
    "\n",
    "            # 3. Guardamos las dimensiones espaciales:\n",
    "            if \"BBOX\" in line.split()[0]:\n",
    "                \n",
    "            \n",
    "            # Nos saltamos todas las líneas hasta encontrar \n",
    "            # el tag del bloque que no interesa:\n",
    "            while True:\n",
    "                df = {}\n",
    "                line = f.readline()\n",
    "                if \"[FILAMENTS]\" in line:\n",
    "                    df[\"findex\"] = i\n",
    "                    df[\"nFilaments\"] = int(f.readline().spli()[0])\n",
    "                    break\n",
    "                    \n",
    "\n",
    "                    \n",
    "            currentfil = findex+1\n",
    "            totallines = 0\n",
    "            print(f\"{currentfil} {totallines}\")\n",
    "            for i in range(2):\n",
    "                totallines += lines[currentfil].split()[2]+1\n",
    "                currentfil += lines[currentfil].split()[2]+1\n",
    "\n",
    "            for i in range(totallines):\n",
    "                fout.write(lines[i+findex])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c3edb7b-d258-4e3e-a46f-ce428d90d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8471 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mextract_filaments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mextract_filaments\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrentfil\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotallines\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     totallines += \u001b[43mlines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrentfil\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m+\u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m     currentfil += lines[currentfil].split()[\u001b[32m2\u001b[39m]+\u001b[32m1\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(totallines):\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "extract_filaments(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f53ecd-6042-4bec-bece-72caa9946e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236a1c5-7644-44a1-8f64-17b1594efe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dc677-81a4-477a-aa9c-73011f802ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
